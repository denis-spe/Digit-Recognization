{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfHF-xOQqNfD"
      },
      "source": [
        "<h1>Digit Recognization With Tensorflow</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CgtQ9TgqNfI"
      },
      "source": [
        "<h2 style=\"font-size: 20px;\">Import Necessary Libraries and packages</h2>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner -q"
      ],
      "metadata": {
        "id": "j9GrB9oJHRej",
        "outputId": "fa6aa4ed-6b08-48b6-b7b2-199297da33e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHfHwR-OqNfK",
        "outputId": "eb7b285e-1b98-481a-bdc1-60010fb142f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow version:  2.9.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras_tuner as kt\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import layers\n",
        "from typing import Union, Tuple, Optional\n",
        "print(\"tensorflow version: \", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "JkD68RMFxBcF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb6P9C3KqNfN"
      },
      "source": [
        "<h2 style=\"font-size: 20px;\">Setting The Notebook</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEjq8QjsqNfN",
        "outputId": "0769e80d-4841-449a-e583-0751cfff86b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Accelerator: 1\n"
          ]
        }
      ],
      "source": [
        "# Setting the graph style\n",
        "plt.rc('figure', autolayout=True)\n",
        "plt.rc(\n",
        "    'axes', titleweight='bold', \n",
        "    titlesize=20, labelweight=700,\n",
        "    labelsize=13\n",
        "    )\n",
        "plt.rc('font', size=15)\n",
        "\n",
        "def set_seeds(seed: int=0):\n",
        "    \"\"\"Sets the Seed into order to get the same result on every code run\"\"\"\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"TF_DETERMINISTIC\"] = str(seed)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
        "\n",
        "# Call the set_seeds with default parameter\n",
        "set_seeds()\n",
        "\n",
        "try:\n",
        "    tfu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    stratagy = tf.distribute.TPUStrategy(tfu)\n",
        "except ValueError:\n",
        "    stratagy = tf.distribute.get_strategy()\n",
        "print(f\"Number of Accelerator: {stratagy.num_replicas_in_sync}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_nrow = 42000\n",
        "test_nrow = 28000\n",
        "training_size = train_nrow - test_nrow\n",
        "BATCH_SIZE_PER_REPLICA = 32\n",
        "batch_size = BATCH_SIZE_PER_REPLICA * stratagy.num_replicas_in_sync\n",
        "steps_per_epoch = training_size // batch_size\n",
        "validation_steps = test_nrow // batch_size"
      ],
      "metadata": {
        "id": "owIluphakaX4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PisWUlUmqNfO"
      },
      "source": [
        "<h2 style=\"font-size: 20px;\">Above Dataset</h2>\n",
        "<p style=\"font-size: 15px;\">\n",
        "The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n",
        "\n",
        "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
        "\n",
        "The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
        "\n",
        "Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWpujn2MqNfP"
      },
      "source": [
        "<h2 style=\"font-size: 20px;\">Load Csv File Into Pandas DataFrame</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjagb9UJqWKc"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    # Mount google files\n",
        "    drive.mount('/gdrive')\n",
        "\n",
        "    # list the files from drive\n",
        "    folder_path = '../gdrive/MyDrive/Datasets/MNIST_data'\n",
        "    \n",
        "except ImportError:\n",
        "    folder_path = '../input/digit-recognizer'\n",
        "\n",
        "os.listdir(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnfQo_yxqNfP"
      },
      "outputs": [],
      "source": [
        "# Data Path\n",
        "train_data_path = folder_path + '/train.csv'\n",
        "test_data_path = folder_path + '/test.csv'\n",
        "\n",
        "# Loading dataset into pandas \n",
        "train_df = pd.read_csv(train_data_path)\n",
        "test_df = pd.read_csv(test_data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfkkDqGdqNfQ"
      },
      "source": [
        "<h2 style=\"font-size: 20px;\">View The Dataframe</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieRawEREqNfR"
      },
      "outputs": [],
      "source": [
        "# First Rows in the train data\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IRzv3OnqNfR"
      },
      "outputs": [],
      "source": [
        "# First Rows in the test data\n",
        "print(test_df.shape)\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iKh19Uu2OEQ"
      },
      "source": [
        "<h2 style=\"font-size: 20px;\">Train Data Split Into Train and Validation data</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqEJl7EA2dV5"
      },
      "outputs": [],
      "source": [
        "mask = np.random.randn(len(train_df)) > 0.8\n",
        "train_data = train_df[mask]\n",
        "valid_data = train_df[~mask]\n",
        "print(f\"Train_data shape: {train_data.shape}\\nValid_data: {valid_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQofPy3lqNfS"
      },
      "source": [
        "<h2 style=\"font-size: 20px;\">Create Dataset Generator</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RMv7k1zqNfT"
      },
      "outputs": [],
      "source": [
        "class DatasetGenerator:\n",
        "    def __init__(self, batch_size: int = batch_size, shuffle: int = False, epoch: int = 10) -> None:\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.epoch = epoch\n",
        "\n",
        "    def __call__(self, df: pd.DataFrame) -> any:\n",
        "        if self.shuffle:\n",
        "            df_ = tf.constant([\n",
        "                row[1:].values / 255\n",
        "                for _, row in df.iterrows()\n",
        "                ])\n",
        "            df_ = tf.reshape(df_, [len(df), 28, 28, 1])\n",
        "            labels = df.iloc[:, 0]\n",
        "            data = tf.data.Dataset.from_tensor_slices((df_, labels)).batch(self.batch_size).shuffle(1000)\n",
        "        else:\n",
        "            df_ = tf.constant([\n",
        "                row.values.reshape(28, 28) /255\n",
        "                for _, row in df.iterrows()\n",
        "                ])\n",
        "            df_ = tf.reshape(df_, [len(df), 28, 28, 1])\n",
        "            data = tf.data.Dataset.from_tensor_slices((df_)).batch(self.batch_size)\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQZNmVfVqNfV"
      },
      "source": [
        "<h2 style=\"font-size: 20px;\">Convert The Data To tensor</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD4Zd62NqNfW"
      },
      "outputs": [],
      "source": [
        "# Change the dataframes to generator \n",
        "train_data = DatasetGenerator(shuffle=True, batch_size=batch_size)(train_data)\n",
        "validation_data = DatasetGenerator(shuffle=True, batch_size=batch_size)(valid_data)\n",
        "test_dataset = DatasetGenerator()(test_df)\n",
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0DWrfItqNfW"
      },
      "source": [
        "<h2 style=\"font-size: 20px;\">Exploring Datasets</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADVM-AAcqNfX"
      },
      "outputs": [],
      "source": [
        "# Exploring One of the image in the train_dataset generator\n",
        "img, label = next(iter(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uup4sof4w7S"
      },
      "outputs": [],
      "source": [
        "print(\"image Datatype: \", type(img[0]))\n",
        "print(\"label data type: \", type(label[1]))\n",
        "print(\"label: \", set(label.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjPGLIZzqNfX"
      },
      "outputs": [],
      "source": [
        "# For exploratory purpose change the image to numpy array\n",
        "np_img = np.array(img)\n",
        "IMG_SHAPE = np_img.shape\n",
        "print(\"Image shape: \", IMG_SHAPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbfPxc_gqNfY"
      },
      "source": [
        "<h2 style=\"font-size: 20px;\">Digits Visualization</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCTp8TCuqNfZ"
      },
      "outputs": [],
      "source": [
        "# Visualizing of the one image in the test_dataset\n",
        "img = next(iter(test_dataset))\n",
        "cmap = plt.get_cmap('magma')\n",
        "plt.imshow(tf.squeeze(img[0]), cmap=cmap)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy9WTFQIqNfZ"
      },
      "outputs": [],
      "source": [
        "# Create Subplot instance\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# Number of columns and rows\n",
        "NROW = 3\n",
        "NCOL = 3\n",
        "\n",
        "# loop over the range of NROW * NCOL starting from 1\n",
        "for i in range(1, NROW * NROW + 1):\n",
        "    fig.add_subplot(NROW, NCOL, i)\n",
        "\n",
        "    # Get the image from the generated index\n",
        "    img, label = next(iter(train_data))\n",
        "\n",
        "    # Plot it\n",
        "    plt.imshow(tf.squeeze(img[0]), \n",
        "        cmap=cmap\n",
        "        )\n",
        "    plt.axis('off')\n",
        "    plt.title(label[0].numpy())\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    ax.set_axis_off()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHCjAPFDqNfb"
      },
      "source": [
        "<h2 style=\"font-size: 20px;\">Modeling</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr9hSvLOqNfb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_model(input_shape, eager: bool = False):\n",
        "    \"\"\"Create a Keras Sequential and complie it\"\"\"\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten(input_shape=input_shape))\n",
        "\n",
        "    model.add(layers.Dense(units=256, activation='relu'))\n",
        "    # model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(rate=0.5))\n",
        "\n",
        "    model.add(layers.Dense(units=256, activation='relu'))\n",
        "    # model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(rate=0.5))\n",
        "\n",
        "    model.add(layers.Dense(units=256, activation='relu'))\n",
        "    # model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "    # Add a output layer\n",
        "    model.add(layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "    if eager:\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='SparseCategoricalCrossentropy',\n",
        "            metrics=['acc'],\n",
        "            run_eagerly=True\n",
        "\n",
        "        )\n",
        "    else:\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='SparseCategoricalCrossentropy',\n",
        "            metrics=['accuracy'],\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# Model instance\n",
        "model = build_model(IMG_SHAPE[1:], eager=True)\n",
        "\n",
        "# Plot the model\n",
        "dot_image_file = '/tmp/model_1.png'\n",
        "keras.utils.plot_model(model, to_file=dot_image_file, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv4bS0qpmOLZ"
      },
      "outputs": [],
      "source": [
        "# Initialize the EarlyStopping class: early_stopping\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    patience=1,\n",
        "    min_delta=0.01,\n",
        "    restore_best_weights=True\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Callback to save the Keras model or model weights at some frequency"
      ],
      "metadata": {
        "id": "zA-8lkezX6W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = \"/tmp/checkpoints\"\n",
        "\n",
        "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ],
      "metadata": {
        "id": "MosPsaLOXykx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce learning rate when a metric has stopped improving"
      ],
      "metadata": {
        "id": "ogk2FU7mhF0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=0, min_lr=0.001, mode='auto', verbose=1)"
      ],
      "metadata": {
        "id": "4Q0XR6xohD7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the tensorboard for debuging and visualizing the model"
      ],
      "metadata": {
        "id": "MVKAECtsxydc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard = callbacks.TensorBoard(\"/tmp/tb_logs\")"
      ],
      "metadata": {
        "id": "WEQW-kfUxxP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKo5JrQLqNfc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Fit the model\n",
        "with stratagy.scope():\n",
        "    model = build_model(IMG_SHAPE[1:])\n",
        "\n",
        "history = model.fit(\n",
        "    train_data, \n",
        "    validation_data=validation_data, \n",
        "    epochs=15,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    batch_size=batch_size,\n",
        "    validation_steps=validation_steps, \n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uEVPYLdHaE8"
      },
      "outputs": [],
      "source": [
        "# Training without early_stopping callback\n",
        "# Fit the model\n",
        "with stratagy.scope():\n",
        "    model = build_model(IMG_SHAPE[1:])\n",
        "\n",
        "history = model.fit(\n",
        "    train_data, \n",
        "    validation_data=validation_data,\n",
        "    epochs=20,\n",
        "    callbacks=[\n",
        "            #    model_checkpoint_callback, \n",
        "                # reduce_lr, \n",
        "            #    tensorboard\n",
        "               ],\n",
        "    batch_size=batch_size,\n",
        "    validation_steps=validation_steps\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"font-size: 20px;\">Model Evaluation</h2>"
      ],
      "metadata": {
        "id": "JNKmLmPLvmbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evalutate = model.evaluate(validation_data)\n",
        "print(f\"Loss: {round(evalutate[0], 4)} - Accurary: {round(evalutate[1], 3)}%\")"
      ],
      "metadata": {
        "id": "xMivAH94viFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b6TZUFwX0_2"
      },
      "outputs": [],
      "source": [
        "# Construct a dataframe with loss and accuracy score\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeN4ZDcMc-kz"
      },
      "outputs": [],
      "source": [
        "def metric_vis(train: str, val: str, ax: plt.axis) -> None:\n",
        "\n",
        "    fig1 = history_df[train].plot(c='pink', ax=ax, marker=\"x\")\n",
        "    fig2 = history_df[val].plot(c='skyblue', ax=ax, marker=\"o\")\n",
        "    ax.set_ylabel(f\"{train} Score\")\n",
        "    ax.set_xlabel(\"Epoch/Iteration\")\n",
        "    ax.set_title(f\"Train And Validation {train}\".title())\n",
        "    # Hide the right and top spines\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.legend([f'{train}', f\"{val}\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7cm6wKhY8iP"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "metric_vis(\"loss\", 'val_loss', ax=ax[0])\n",
        "metric_vis(\"accuracy\", 'val_accuracy', ax=ax[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir /tmp/tb_logs"
      ],
      "metadata": {
        "id": "sydxnpXZw6sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hypeparameter Tuning**"
      ],
      "metadata": {
        "id": "8kCePuW6E6XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp: kt.HyperParameters) -> keras.Sequential:\n",
        "    \"\"\"\n",
        "    Create a Keras Sequential and complie it\n",
        "    :param hp for hypeparameter values:\n",
        "    :return tf.keras.Sequential model:\n",
        "    \"\"\"\n",
        "    # Construct a model\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten(input_shape=IMG_SHAPE[1:]))\n",
        "\n",
        "    # Tune the number of units\n",
        "    # Ranging from 32 to 512\n",
        "    hp_units = hp.Int(\"units\", min_value=32, max_value=512, step=32)\n",
        "    \n",
        "    # Tune the number of drop out rate\n",
        "    # Rate will be in range of 0.1 to 0.5\n",
        "    hp_rate = hp.Float('rate', min_value=0.1, max_value=1)\n",
        "\n",
        "    # Tune the activation functions\n",
        "    hp_act = hp.Choice('activation', values=[\n",
        "                'relu', 'elu', 'gelu',\n",
        "                'linear', 'selu', 'swish',\n",
        "                'tanh'\n",
        "                ])\n",
        "\n",
        "    # *** looking for prefect number of layers ***\n",
        "    hidden_layers = hp.Choice(\"hidden_layers\", values=[\"two\", \"three\", \"four\", \"six\"])\n",
        "\n",
        "    if hidden_layers == \"two\":\n",
        "        with hp.conditional_scope(\"hidden_layers\", [\"two\"]):\n",
        "            # first Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "            # second Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "    \n",
        "    if hidden_layers == \"three\":\n",
        "        with hp.conditional_scope(\"hidden_layers\", [\"three\"]):\n",
        "            # first Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "            # second Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "            # three Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "    \n",
        "    if hidden_layers == \"four\":\n",
        "        with hp.conditional_scope(\"hidden_layers\", [\"four\"]):\n",
        "            # first Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "            # second Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "            # third Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "            # fourth Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "    \n",
        "    if hidden_layers == \"six\":\n",
        "        with hp.conditional_scope(\"hidden_layers\", [\"six\"]):\n",
        "            # first Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "            # second Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "            # third Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "            # fourth Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "            # fifth Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "            # sixth Hidden layer\n",
        "            model.add(layers.Dense(units=hp_units, activation=hp_act))\n",
        "            # Drop out layer\n",
        "            model.add(layers.Dropout(rate=hp_rate))\n",
        "            # Normalization data\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "    # Add a output layer\n",
        "    model.add(layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "    \n",
        "    # Compile the model\n",
        "    # Tune the learning rate for optimizer\n",
        "    # Ranging from 0.01, 0.001, or 0.0001\n",
        "    hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-3, 1e-4, 1e-2, ])\n",
        "\n",
        "    # **** looking for best optimizer *****\n",
        "\n",
        "    hp_optimizer = hp.Choice(\"optimizer\", values=[\n",
        "                        \"Adam\", \"RMSprop\", \n",
        "                        \"Adamax\", \"Nadam\",\n",
        "                        \"SGD\"\n",
        "                        ])\n",
        "\n",
        "    if hp_optimizer == \"Adam\":\n",
        "        with hp.conditional_scope(\"optimizer\", [\"Adam\"]):\n",
        "            model.compile(\n",
        "                optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=[\"accuracy\"]\n",
        "            )\n",
        "\n",
        "    if hp_optimizer == \"RMSprop\":\n",
        "        with hp.conditional_scope(\"optimizer\", [\"RMSprop\"]):\n",
        "            model.compile(\n",
        "                optimizer=keras.optimizers.RMSprop(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=[\"accuracy\"]\n",
        "            )\n",
        "\n",
        "    if hp_optimizer == \"Adamax\":\n",
        "        with hp.conditional_scope(\"optimizer\", [\"Adamax\"]):\n",
        "            model.compile(\n",
        "                optimizer=keras.optimizers.Adamax(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=[\"accuracy\"]\n",
        "            )\n",
        "\n",
        "    if hp_optimizer == \"Nadam\":\n",
        "        with hp.conditional_scope(\"optimizer\", [\"Nadam\"]):\n",
        "            model.compile(\n",
        "                optimizer=keras.optimizers.Adamax(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=[\"accuracy\"]\n",
        "            )\n",
        "\n",
        "    if hp_optimizer == \"SGD\":\n",
        "        with hp.conditional_scope(\"optimizer\", [\"SGD\"]):\n",
        "            model.compile(\n",
        "                optimizer=keras.optimizers.Adamax(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=[\"accuracy\"]\n",
        "            )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "a4k89vwJEVYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate tuner and preform hypertuning\n",
        "tuner = kt.Hyperband(\n",
        "    model_builder,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_epochs=10,\n",
        "    directory=\"/tmp/hypeparameters\",\n",
        "    project_name=\"intro_to_hypetuning\"\n",
        ")"
      ],
      "metadata": {
        "id": "odWuSsQoNRVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the earlystopping instance\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "\n",
        "# tuner.search(\n",
        "#     train_data,\n",
        "#     validation_data=validation_data,\n",
        "#     epochs=8,\n",
        "#     callbacks=[early_stopping]\n",
        "# )"
      ],
      "metadata": {
        "id": "8Y6Wz3IpO5WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tuner.results_summary()"
      ],
      "metadata": {
        "id": "D_iTe7wbOGT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_hps = tuner.get_best_hyperparameters(num_trials=5)\n",
        "# print(f\"\"\"\n",
        "# The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "# layer is {best_hps[0].get('units')} and the optimal learning rate for the optimizer\n",
        "# is {best_hps[0].get('learning_rate')}.\n",
        "# \"\"\")"
      ],
      "metadata": {
        "id": "EBmKIyXnR0ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# built_model = tuner.hypermodel.build(best_hps[0])\n",
        "\n",
        "# # Fit the best model from the hypetuning\n",
        "# built_model.fit(\n",
        "#     train_data, \n",
        "#     validation_data=validation_data,\n",
        "#     epochs=20,\n",
        "#     callbacks=[\n",
        "#                model_checkpoint_callback, \n",
        "#                reduce_lr, \n",
        "#                tensorboard\n",
        "#                ],\n",
        "#     # Made same improve with these two code\n",
        "#     batch_size=batch_size,\n",
        "#     validation_steps=validation_steps\n",
        "#     )"
      ],
      "metadata": {
        "id": "Ep5oYX2aNHPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building A Model with Convolutional Layers**"
      ],
      "metadata": {
        "id": "nV9YC4a5QUe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using one since the images are in gray scale.\n",
        "input_shape = [64, 28, 28, 1]\n",
        "input_shape"
      ],
      "metadata": {
        "id": "Zkr8QODKy-mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvModel(keras.Model):\n",
        "\n",
        "    # Constructor .....\n",
        "    def __init__(self):\n",
        "        super(ConvModel, self).__init__()\n",
        "\n",
        "        # Base Blocks ..........................\n",
        "\n",
        "        # First Block .....\n",
        "        self.first_conv2d = layers.Conv2D(\n",
        "            filters=1024,\n",
        "            activation='leaky_relu',\n",
        "            kernel_size=3,\n",
        "            input_shape = input_shape,\n",
        "            name='First_conv2D'\n",
        "            )\n",
        "        self.max_pool_1 = layers.MaxPooling2D(name='MaxPooling2D_1')\n",
        "        # Drop same pixels ....\n",
        "        self.dropout_1 = layers.Dropout(.5, name='Dropout_1')\n",
        "\n",
        "        # Second Block .....\n",
        "        self.second_conv2d = layers.Conv2D(\n",
        "            filters=512,\n",
        "            activation='relu',\n",
        "            kernel_size=3,\n",
        "            name='Second_conv2D'\n",
        "           )\n",
        "        self.max_pool_2 = layers.MaxPooling2D(name='MaxPooling2D_2')\n",
        "        # Drop same pixels ....\n",
        "        self.dropout_2 = layers.Dropout(.5, name='Dropout_2')\n",
        "\n",
        "        # Third Block .....\n",
        "        self.third_conv2d = layers.Conv2D(\n",
        "            filters=256,\n",
        "            activation='leaky_relu',\n",
        "            kernel_size=3,\n",
        "            name='Third_conv2D'\n",
        "           )\n",
        "        # self.avg_pooling_3 = layers.AveragePooling2D(name='AveragePooling2D_1')\n",
        "        # Drop same pixels ....\n",
        "        self.dropout_3 = layers.Dropout(.5, name='Dropout_3')\n",
        "\n",
        "\n",
        "        # Forth Block .....\n",
        "        self.forth_conv2d = layers.Conv2D(\n",
        "            filters=128,\n",
        "            activation='relu',\n",
        "            kernel_size=3,\n",
        "            name='forth_conv2D'\n",
        "           )\n",
        "        # self.max_pool_4 = layers.MaxPooling2D(name='MaxPooling2D_4')\n",
        "        # Drop same pixels ....\n",
        "        self.dropout_4 = layers.Dropout(.6, name='Dropout_4')\n",
        "\n",
        "\n",
        "        # Head Block ............................\n",
        "\n",
        "        # Data Flatten Layer .....\n",
        "        self.flatten = layers.Flatten()\n",
        "\n",
        "        # Hidden layer ...\n",
        "        self.first_hidden_dense = layers.Dense(\n",
        "            units=128, activation='leaky_relu', name='Hidden_layer_1'\n",
        "            )\n",
        "\n",
        "        # Output layer .....\n",
        "        self._output = layers.Dense(\n",
        "            units=10, activation='softmax', name='Output_layer'\n",
        "            )\n",
        "\n",
        "\n",
        "    def call(self, X) -> tf.Tensor:\n",
        "        # Base Blocks ..........................\n",
        "\n",
        "        # First Block .....\n",
        "        first_conv2d = self.first_conv2d(X)\n",
        "        max_pool_1 = self.max_pool_1(first_conv2d)\n",
        "        # Drop same pixels ....\n",
        "        dropout_1 = self.dropout_1(max_pool_1)\n",
        "\n",
        "        # Second Block .....\n",
        "        second_conv2d = self.second_conv2d(dropout_1)\n",
        "        max_pool_2 = self.max_pool_2(second_conv2d)\n",
        "        # Drop same pixels ....\n",
        "        dropout_2 = self.dropout_2(max_pool_2)\n",
        "\n",
        "        # Third Block .....\n",
        "        third_conv2d = self.third_conv2d(dropout_2)\n",
        "        # avg_pool_1 = self.avg_pooling_3(third_conv2d)\n",
        "        # Drop same pixels ....\n",
        "        dropout_3 = self.dropout_3(third_conv2d)\n",
        "\n",
        "        # Forth Block .....\n",
        "        forth_conv2d = self.forth_conv2d(dropout_3)\n",
        "        # max_pool_4 = self.max_pool_4(forth_conv2d)\n",
        "        # Drop same pixels ....\n",
        "        dropout_4 = self.dropout_4(forth_conv2d)\n",
        "\n",
        "\n",
        "\n",
        "        # Head Block ............................\n",
        "\n",
        "        # Data Flatten Layer .....\n",
        "        flatten = self.flatten(dropout_4)\n",
        "\n",
        "        # Hidden layers .....\n",
        "        first_hidden_dense = self.first_hidden_dense(flatten)\n",
        "\n",
        "        # Output layer .....\n",
        "        _output = self._output(first_hidden_dense)\n",
        "        \n",
        "        return _output\n"
      ],
      "metadata": {
        "id": "86Y-QBzAQTrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with stratagy.scope():\n",
        "    # Instantiate the model object .....\n",
        "    conv_model = ConvModel()\n",
        "\n",
        "    # Build the model .....\n",
        "    conv_model.build(input_shape)\n",
        "\n",
        "    # Get the Model summary ...\n",
        "    conv_model.summary()\n",
        "\n",
        "    # Compile the model ...\n",
        "    conv_model.compile(\n",
        "            optimizer='adam',\n",
        "            loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "            metrics=['accuracy'],\n",
        "        )\n",
        "\n",
        "# Fit the Conv model ....\n",
        "conv_model.fit(\n",
        "    train_data, \n",
        "    validation_data=validation_data, \n",
        "    epochs=20,\n",
        "    # callbacks=[\n",
        "    #            model_checkpoint_callback, \n",
        "    #            reduce_lr, \n",
        "    #            tensorboard\n",
        "    #            ],\n",
        "    batch_size=batch_size\n",
        "    )"
      ],
      "metadata": {
        "id": "tIUZUiZyk9DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tune Convalutional Model"
      ],
      "metadata": {
        "id": "KOPXjp3F2x52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cov_model(hp: kt.HyperParameters):\n",
        "    # Tune the convolutional model filters.\n",
        "    filters = hp.Choice(name='filter', values=[64, 128, 256, 512])\n",
        "\n",
        "    # Instantite the Sequential model.\n",
        "    conv_model = keras.Sequential()\n",
        "\n",
        "    # The base.\n",
        "    # Adding the First convolutional layer.\n",
        "    conv_model.add(layers.Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=3,\n",
        "        activation='relu',\n",
        "        input_shape=(28, 28, 1)\n",
        "    ))\n",
        "\n",
        "    # The Head.\n",
        "    # Flatten the data.\n",
        "    conv_model.add(layers.Flatten())\n",
        "\n",
        "    # Adding the First Dense layer.\n",
        "    conv_model.add(layers.Dense(\n",
        "        units=128,\n",
        "        activation='softmax'\n",
        "    ))\n",
        "\n",
        "    # Compile the model.\n",
        "    conv_model.compile(\n",
        "        optimizer='rmsprop',\n",
        "        loss='sparse_categorical_crossentropy'\n",
        "    )\n",
        "\n",
        "    return conv_model\n"
      ],
      "metadata": {
        "id": "KTZCVEUT2wzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the earlystopping instance\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "\n",
        "# Instantiate tuner and preform hypertuning\n",
        "conv_tuner = kt.RandomSearch(\n",
        "    build_cov_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    # max_epochs=10,\n",
        "    directory=\"/tmp/convolutional_model_hypeparameters\",\n",
        "    project_name=\"tuning_convolutional_model\"\n",
        ")\n",
        "\n",
        "# Initialize the earlystopping instance\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_accuracy')\n",
        "\n",
        "conv_tuner.search(\n",
        "    train_data,\n",
        "    # validation_data=validation_data,\n",
        "    epochs=8,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "NDQ-PBg7DfAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "efwpJtXV5DFz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ivZTEHFWwsh"
      },
      "source": [
        "<h2 style=\"font-size: 20px;\">Submission</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdhSM8kMqNfd"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "prob  = conv_model.predict(test_dataset)\n",
        "# Get the highest value from probability\n",
        "pred = tf.argmax(prob, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WE9oJ-iPWVD"
      },
      "outputs": [],
      "source": [
        "submission=pd.read_csv(folder_path + '/sample_submission.csv')\n",
        "submission['Label']=pred\n",
        "submission.to_csv('prediction20_conv.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "digit_recognization_tf.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "2d3cd06894a6a6978ad5a2c20767bf28db0a15e6b1f133abf29af83db06c1725"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}